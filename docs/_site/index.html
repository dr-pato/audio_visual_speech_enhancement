<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments | Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi, Luciano Fadiga, Leonardo Badino University of Modena and Reggio Emilia and Istituto Italiano di Tecnologia</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi, Luciano Fadiga, Leonardo Badino University of Modena and Reggio Emilia and Istituto Italiano di Tecnologia" />
<meta property="og:description" content="Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi, Luciano Fadiga, Leonardo Badino University of Modena and Reggio Emilia and Istituto Italiano di Tecnologia" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments" />
<script type="application/ld+json">
{"headline":"Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments","url":"http://localhost:4000/","name":"Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments","description":"Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi, Luciano Fadiga, Leonardo Badino University of Modena and Reggio Emilia and Istituto Italiano di Tecnologia","@type":"WebSite","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/assets/css/style.css?v=587c6138b332665eee05b2dcf48d3084cc54d1b6">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement in Multi-Talker Environments</h1>
      <h2 class="project-tagline">Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi, Luciano Fadiga, Leonardo Badino <br> University of Modena and Reggio Emilia and Istituto Italiano di Tecnologia</h2>
      
        <a href="http://github.com/dr-pato/audio_visual_speech_enhancement" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
      <h2 id="abstract">Abstract</h2>
<p>We address the problem of enhancing the speech of a speaker of interest in a cocktail party scenario when visual information of the speaker of interest is available. Contrary to most previous studies, we do not learn visual features on the typically small audio-visual datasets, but use an already available face landmark detector (trained on a separate image dataset). The landmarks are used by LSTM-based models to generate time-frequency masks which are applied to the acoustic mixed-speech spectrogram. Results show that: (i) landmark motion features are very effective features for this task, (ii) similarly to previous work, reconstruction of the target speakerâ€™s spectrogram mediated by masking is significantly more accurate than direct spectrogram reconstruction, and (iii) the best masks depend on both motion landmark features and the input mixed-speech spectrogram. To the best of our knowledge, our proposed models are the first models trained and evaluated on the limited size GRID and TCD-TIMIT datasets, that achieve speaker-independent speech enhancement in a multi-talker setting.</p>

<div align="center">
<iframe width="800" height="450" src="https://www.youtube.com/embed/_him5fsvXyM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</div>

<h2 id="demos">Demos</h2>
<p>The following videos contains severals examples of enhanced speech generated by models proposed in our paper.</p>

<h3 id="grid-dataset">GRID dataset</h3>

<table>
  <thead>
    <tr>
      <th>2-Speakers Mix</th>
      <th>3-Speakers Mix</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&lt;iframe width="400" height="333" src="/videos/grid_2spk.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt; &lt;/iframe&gt;</td>
      <td>&lt;iframe width="400" height="333" src="/videos/grid_3spk.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt; &lt;/iframe&gt;</td>
    </tr>
  </tbody>
</table>



      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="http://github.com/dr-pato/audio_visual_speech_enhancement">audio_visual_speech_enhancement</a> is maintained by <a href="http://github.com/dr-pato">dr-pato</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
